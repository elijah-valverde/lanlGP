{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Proccess (GP) on M1 and S2 Site Data\n",
    "We wish to determine the seasonal and diurnal cycles of supermicron aerosols/bioaerosols. In this notebook we focus on the seasonal trends. We will split the data up by seasons and fit a GP on a subset of the data. Using a confidence interval, we will determine if the data indicates seasonal cycles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.special import kv, gamma\n",
    "import itertools\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interactive\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"2\"\n",
    "\n",
    "\n",
    "\n",
    "def data_import(path1, delimiter):\n",
    "    #locate file path and import data\n",
    "    if delimiter == 'none':\n",
    "        file1 = pd.read_csv(path1)\n",
    "    else:   \n",
    "        file1 = pd.read_csv(path1, delimiter=delimiter)\n",
    "    return file1\n",
    "\n",
    "def data_convert(file1, time_column_name: str):\n",
    "    #convert timestamps to datetime format\n",
    "    file1[time_column_name] = pd.to_datetime(file1[time_column_name])\n",
    "   \n",
    "    #handle missing values (NaN); fill with mean value\n",
    "    file1.fillna(file1.mean(), inplace=True)\n",
    "\n",
    "    return file1\n",
    "\n",
    "def data_frequency(file1, desired_frequency: str, time_column_name: str):\n",
    "    if desired_frequency == 'ten_minute':\n",
    "        file1 = file1.resample('10T', on=time_column_name).mean()\n",
    "    elif desired_frequency == '4_hourly':\n",
    "        file1 = file1.resample('4h', on=time_column_name).mean()\n",
    "    elif desired_frequency == 'hourly':\n",
    "        file1 = file1.resample('h', on=time_column_name).mean()\n",
    "    elif desired_frequency == 'twelve_hourly':\n",
    "        file1 = file1.resample('12h', on=time_column_name).mean()\n",
    "    elif desired_frequency == 'daily':\n",
    "        file1 = file1.resample('D', on=time_column_name).mean()\n",
    "\n",
    "    # Ensure the index is datetime\n",
    "    file1.index = pd.to_datetime(file1.index)\n",
    "    \n",
    "    # Reset the index and name it 'Time(UTC)'\n",
    "    file1.reset_index(inplace=True)\n",
    "    file1.rename(columns={file1.index.name: time_column_name}, inplace=True)\n",
    "\n",
    "    return file1\n",
    "\n",
    "\n",
    "def time_to_sincos(df):\n",
    "    sin_values = []\n",
    "    cos_values = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        sin_values.append(np.sin((2 * np.pi * i) / 365.25))\n",
    "        cos_values.append(np.cos((2 * np.pi * i) / 365.25))\n",
    "        \n",
    "    df['Time_sin'] = sin_values\n",
    "    df['Time_cos'] = cos_values\n",
    "    \n",
    "    return df\n",
    "\n",
    "#trim data to desired collumns\n",
    "def file_trim(file1, desired_collumns):\n",
    "    file1 = file1[desired_collumns]\n",
    "    return file1\n",
    "\n",
    "#separate data by seasons\n",
    "def data_split(file1, time_column_name: str):\n",
    "    winter = file1[(file1[time_column_name] >= '2022-12-21') & (file1[time_column_name] < '2023-03-20')]\n",
    "    spring = file1[(file1[time_column_name] >= '2023-03-20') & (file1[time_column_name] < '2023-06-21')]\n",
    "    summer = file1[(file1[time_column_name] >= '2022-06-21') & (file1[time_column_name] < '2022-09-23')]\n",
    "    autunm = file1[(file1[time_column_name] >= '2022-09-23') & (file1[time_column_name] < '2022-12-21')]\n",
    "    return winter, spring, summer, autunm\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S2 site data importing and modeling \n",
    "Here we import and handle all of the data. s2 first, split the varying frequencies and seasons, then M1 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing s2 and m1 data sets\n",
    "s2_site_data= data_import(\"C:\\\\Users\\\\396760\\\\lanl\\\\data\\\\ARMSAILS2_cleaned.csv\", 'none')\n",
    "m1_site_data = data_import(\"C:\\\\Users\\\\396760\\\\lanl\\\\data\\\\ARMSAILM1_cleaned.csv\", 'none')\n",
    "\n",
    "#data conversion and handling for m1, s2 data\n",
    "s2_site_data, m1_site_data = data_convert(s2_site_data, 'Time(UTC)'), data_convert(m1_site_data, 'Time(UTC)')\n",
    "\n",
    "#defining collumns and imp feature variables\n",
    "collumns = ['Time(UTC)', 'sample_rh_pct', 'sample_temp_C', 'pm_1_ug_per_m3', 'Time_sin', 'Time_cos']\n",
    "collumns1 = ['Time(UTC)', 'sample_rh_pct', 'sample_temp_C', 'pm_25_ug_per_m3', 'Time_sin', 'Time_cos']\n",
    "features = ['sample_rh_pct', 'sample_temp_C', 'Time_sin', 'Time_cos']\n",
    "s2_target_pm1 = m1_target_pm1 = ['pm_1_ug_per_m3']\n",
    "s2_target_pm25 = m1_target_pm25 = ['pm_25_ug_per_m3']\n",
    "s2_target_pm10 = m1_target_pm10 = ['pm_10_ug_per_m3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining datasets of differing frequencies in the following order : daily, 12 hourly, 4 hourly and hourly, \n",
    "s2_site_daily, m1_site_daily = data_frequency(s2_site_data, 'daily', 'Time(UTC)'), data_frequency(m1_site_data, 'daily', 'Time(UTC)')\n",
    "s2_site_12hourly, m1_site_12hourly = data_frequency(s2_site_data, 'twelve_hourly', 'Time(UTC)'), data_frequency(m1_site_data, 'twelve_hourly', 'Time(UTC)')\n",
    "s2_site_4hourly, m1_site_4hourly = data_frequency(s2_site_data, '4_hourly', 'Time(UTC)'), data_frequency(m1_site_data, '4_hourly', 'Time(UTC)')\n",
    "s2_site_hourly, m1_site_hourly = data_frequency(s2_site_data, 'hourly', 'Time(UTC)'), data_frequency(m1_site_data, 'hourly', 'Time(UTC)')\n",
    "\n",
    "#time to sin cos conversion\n",
    "s2_site_daily, m1_site_daily = time_to_sincos(s2_site_daily), time_to_sincos(m1_site_daily)\n",
    "s2_site_12hourly, m1_site_12hourly = time_to_sincos(s2_site_12hourly), time_to_sincos(m1_site_12hourly)\n",
    "s2_site_4hourly, m1_site_4hourly = time_to_sincos(s2_site_4hourly), time_to_sincos(m1_site_4hourly)\n",
    "s2_site_hourly, m1_site_hourly = time_to_sincos(s2_site_hourly), time_to_sincos(m1_site_hourly)\n",
    "\n",
    "#trimming data to desired collumns\n",
    "s2_site_daily, m1_site_daily = file_trim(s2_site_daily, collumns), file_trim(m1_site_daily, collumns)\n",
    "s2_site_12hourly, m1_site_12hourly = file_trim(s2_site_12hourly, collumns), file_trim(m1_site_12hourly, collumns)\n",
    "s2_site_4hourly, m1_site_4hourly = file_trim(s2_site_4hourly, collumns), file_trim(m1_site_4hourly, collumns)\n",
    "s2_site_hourly, m1_site_hourly = file_trim(s2_site_hourly, collumns), file_trim(m1_site_hourly, collumns)\n",
    "\n",
    "#splitting data by seasons\n",
    "\n",
    "#daily\n",
    "s2_winter_daily, s2_spring_daily, s2_summer_daily, s2_autunm_daily = data_split(s2_site_daily, 'Time(UTC)')\n",
    "m1_winter_daily, m1_spring_daily, m1_summer_daily, m1_autunm_daily = data_split(m1_site_daily, 'Time(UTC)')\n",
    "#12 hourly\n",
    "s2_winter_12hourly, s2_spring_12hourly, s2_summer_12hourly, s2_autunm_12hourly = data_split(s2_site_12hourly, 'Time(UTC)')\n",
    "m1_winter_12hourly, m1_spring_12hourly, m1_summer_12hourly, m1_autunm_12hourly = data_split(m1_site_12hourly, 'Time(UTC)')\n",
    "#4 hourly\n",
    "s2_winter_4hourly, s2_spring_4hourly, s2_summer_4hourly, s2_autunm_4hourly = data_split(s2_site_4hourly, 'Time(UTC)')\n",
    "m1_winter_4hourly, m1_spring_4hourly, m1_summer_4hourly, m1_autunm_4hourly = data_split(m1_site_4hourly, 'Time(UTC)')\n",
    "#hourly\n",
    "s2_winter_hourly, s2_spring_hourly, s2_summer_hourly, s2_autunm_hourly = data_split(s2_site_hourly, 'Time(UTC)')\n",
    "m1_winter_hourly, m1_spring_hourly, m1_summer_hourly, m1_autunm_hourly = data_split(m1_site_hourly, 'Time(UTC)')\n",
    "\n",
    "#plots for s2 data sites at varying frequencies (PM1)\n",
    "\n",
    "#daily\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(s2_winter_daily['Time(UTC)'], s2_winter_daily['pm_1_ug_per_m3'], label='Winter')\n",
    "plt.plot(s2_spring_daily['Time(UTC)'], s2_spring_daily['pm_1_ug_per_m3'], label='Spring')\n",
    "plt.plot(s2_summer_daily['Time(UTC)'], s2_summer_daily['pm_1_ug_per_m3'], label='Summer')\n",
    "plt.plot(s2_autunm_daily['Time(UTC)'], s2_autunm_daily['pm_1_ug_per_m3'], label='Autunm')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#12 hourly\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(s2_winter_12hourly['Time(UTC)'], s2_winter_12hourly['pm_1_ug_per_m3'], label='Winter')\n",
    "plt.plot(s2_spring_12hourly['Time(UTC)'], s2_spring_12hourly['pm_1_ug_per_m3'], label='Spring')\n",
    "plt.plot(s2_summer_12hourly['Time(UTC)'], s2_summer_12hourly['pm_1_ug_per_m3'], label='Summer')\n",
    "plt.plot(s2_autunm_12hourly['Time(UTC)'], s2_autunm_12hourly['pm_1_ug_per_m3'], label='Autunm')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#4 hourly\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(s2_winter_4hourly['Time(UTC)'], s2_winter_4hourly['pm_1_ug_per_m3'], label='Winter')\n",
    "plt.plot(s2_spring_4hourly['Time(UTC)'], s2_spring_4hourly['pm_1_ug_per_m3'], label='Spring')\n",
    "plt.plot(s2_summer_4hourly['Time(UTC)'], s2_summer_4hourly['pm_1_ug_per_m3'], label='Summer')\n",
    "plt.plot(s2_autunm_4hourly['Time(UTC)'], s2_autunm_4hourly['pm_1_ug_per_m3'], label='Autunm')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#hourly\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(s2_winter_hourly['Time(UTC)'], s2_winter_hourly['pm_1_ug_per_m3'], label='Winter')\n",
    "plt.plot(s2_spring_hourly['Time(UTC)'], s2_spring_hourly['pm_1_ug_per_m3'], label='Spring')\n",
    "plt.plot(s2_summer_hourly['Time(UTC)'], s2_summer_hourly['pm_1_ug_per_m3'], label='Summer')\n",
    "plt.plot(s2_autunm_hourly['Time(UTC)'], s2_autunm_hourly['pm_1_ug_per_m3'], label='Autunm')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualizing our choice of kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_kernel_function_space(kernel, x_range, n_functions, **kernel_params):\n",
    "    # Create a grid of x values\n",
    "    X = np.atleast_2d(x_range).T\n",
    "\n",
    "    # Compute the kernel matrix\n",
    "    K = kernel(X, X, **kernel_params)\n",
    "\n",
    "    # Clear the plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Generate functions\n",
    "    for _ in range(n_functions):\n",
    "        # Draw samples from a multivariate normal distribution\n",
    "        f = multivariate_normal.rvs(mean=10*np.ones(X.shape[0]), cov=K)\n",
    "        \n",
    "        # Plot the function\n",
    "        plt.plot(x_range, f)\n",
    "\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Function Value')\n",
    "    plt.title(f'Functions Sampled from {kernel.__name__}')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def white_noise_kernel(x1, x2, variance=5.0):\n",
    "    # Compute the kernel\n",
    "    if np.array_equal(x1, x2):\n",
    "        return variance * np.eye(x1.shape[0])\n",
    "    else:\n",
    "        return np.zeros((x1.shape[0], x2.shape[0]))\n",
    "    \n",
    "def rbf_kernel(x1, x2, length_scale=1.0, variance=1.0):\n",
    "    # Compute the pairwise squared distances between the inputs\n",
    "    sq_dists = cdist(x1, x2, 'sqeuclidean')\n",
    "    \n",
    "    # Compute the kernel\n",
    "    return variance * np.exp(-0.5 * sq_dists / length_scale**2)\n",
    "\n",
    "def periodic_kernel(x1, x2, length_scale=1.0, variance=1.0, period=1.0):\n",
    "    # Compute the pairwise squared distances between the inputs\n",
    "    sq_dists = cdist(x1, x2, 'sqeuclidean')\n",
    "    \n",
    "    # Compute the kernel\n",
    "    return variance * np.exp(-2 * np.sin(np.pi * np.sqrt(sq_dists) / period)**2 / length_scale**2)\n",
    "\n",
    "def matern_kernel(x, x_star, length_scale=1.0, nu=0.5, variance=1.0):\n",
    "    dists = cdist(x / length_scale, x_star / length_scale, metric='euclidean')\n",
    "    \n",
    "    if nu == 0.5:\n",
    "        K = np.exp(-dists)\n",
    "    elif nu == 1.5:\n",
    "        sqrt3 = np.sqrt(3)\n",
    "        K = (1.0 + sqrt3 * dists) * np.exp(-sqrt3 * dists)\n",
    "    elif nu == 2.5:\n",
    "        sqrt5 = np.sqrt(5)\n",
    "        K = (1.0 + sqrt5 * dists + (5.0 / 3.0) * (dists ** 2)) * np.exp(-sqrt5 * dists)\n",
    "\n",
    "    return variance * K\n",
    "\n",
    "def sum_kernel(x1, x2, variance, length_scale, nu, period):\n",
    "    return white_noise_kernel(x1, x2, variance=variance) + matern_kernel(x1, x2, variance=variance, length_scale=length_scale, nu=nu) + periodic_kernel(x1, x2, variance=variance, length_scale=length_scale, period=period)\n",
    "\n",
    "x_space = np.linspace(0, 10, 100)\n",
    "num_of_functions = 3\n",
    "\n",
    "# Create sliders for kernel parameters\n",
    "def plot_white_noise_kernel(variance):\n",
    "    visualize_kernel_function_space(white_noise_kernel, x_space, num_of_functions, variance=variance)\n",
    "\n",
    "def plot_rbf_kernel(length_scale, variance):\n",
    "    visualize_kernel_function_space(rbf_kernel, x_space, num_of_functions, length_scale=length_scale, variance=variance)\n",
    "\n",
    "def plot_periodic_kernel(length_scale, variance, period):\n",
    "    visualize_kernel_function_space(periodic_kernel, x_space, num_of_functions, length_scale=length_scale, variance=variance, period=period)\n",
    "\n",
    "def plot_matern_kernel(length_scale, nu, variance):\n",
    "    visualize_kernel_function_space(matern_kernel, x_space, num_of_functions, length_scale=length_scale, nu=nu, variance=variance)\n",
    "\n",
    "def plot_sum_kernel(variance, length_scale, nu, period):\n",
    "    visualize_kernel_function_space(sum_kernel, x_space, num_of_functions, variance=variance, length_scale=length_scale, nu=nu, period=period)\n",
    "\n",
    "# Create interactive widgets\n",
    "interactive_plot_white_noise = interactive(plot_white_noise_kernel, variance=widgets.FloatSlider(min=0.1, max=10.0, step=0.1, value=2.0))\n",
    "interactive_plot_rbf = interactive(plot_rbf_kernel, length_scale=widgets.FloatSlider(min=0.1, max=10.0, step=0.1, value=1.0), variance=widgets.FloatSlider(min=0.1, max=10.0, step=0.1, value=1.0))\n",
    "interactive_plot_periodic = interactive(plot_periodic_kernel, length_scale=widgets.FloatSlider(min=0.1, max=10.0, step=0.1, value=1.0), variance=widgets.FloatSlider(min=0.1, max=10.0, step=0.1, value=1.0), period=widgets.FloatSlider(min=0.1, max=10.0, step=0.1, value=1.0))\n",
    "interactive_plot_matern = interactive(plot_matern_kernel, length_scale=widgets.FloatSlider(min=0.1, max=10.0, step=0.1, value=1.0), nu=widgets.FloatSlider(min=0.5, max=2.5, step=1.0, value=0.5), variance=widgets.FloatSlider(min=0.1, max=10.0, step=0.1, value=1.0))\n",
    "interactive_plot_sum = interactive(plot_sum_kernel, variance=widgets.FloatSlider(min=0.1, max=10.0, step=0.1, value=1.0), length_scale=widgets.FloatSlider(min=0.1, max=10.0, step=0.1, value=1.0), nu=widgets.FloatSlider(min=0.5, max=2.5, step=1.0, value=0.5), period=widgets.FloatSlider(min=0.1, max=10.0, step=0.1, value=1.0))\n",
    "display(interactive_plot_white_noise)\n",
    "display(interactive_plot_rbf)\n",
    "display(interactive_plot_periodic)\n",
    "display(interactive_plot_matern)\n",
    "display(interactive_plot_sum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_subset(X, y, n):\n",
    "\n",
    "    # Number of samples to select\n",
    "    num_samples = int((n / 100) * X.shape[0])\n",
    "\n",
    "    # Generate a random subset of indices\n",
    "    subset_indices = np.random.choice(X.shape[0], num_samples, replace=False)\n",
    "\n",
    "    # Extract the subset of the data, for y\n",
    "    X_subset = X[subset_indices, :]\n",
    "    y_subset = y[subset_indices, :]\n",
    "\n",
    "   \n",
    "\n",
    "    return X_subset, y_subset\n",
    "\n",
    "def covariance_matrix(x, x_star, kernel, **kernel_params):\n",
    "    return kernel(x, x_star, **kernel_params)\n",
    "\n",
    "def run_gp_regression(X, y, X_star, kernel, **kernel_params):\n",
    "\n",
    "    # Compute the covariance matrices\n",
    "    K = covariance_matrix(X, X, kernel, **kernel_params)\n",
    "    K_star = covariance_matrix(X, X_star, kernel, **kernel_params)\n",
    "    K_star_star = covariance_matrix(X_star, X_star, kernel, **kernel_params)\n",
    "    \n",
    "    # Compute the Cholesky decomposition of K\n",
    "    L = np.linalg.cholesky(K + 1e-6 * np.eye(len(X)))\n",
    "    \n",
    "    # Compute the mean of the posterior predictive distribution\n",
    "    alpha = np.linalg.solve(L.T, np.linalg.solve(L, y))\n",
    "    f_star_mean = K_star.T @ alpha\n",
    "    \n",
    "    # Compute the variance of the posterior predictive distribution\n",
    "    v = np.linalg.solve(L, K_star)\n",
    "    f_star_var = K_star_star - v.T @ v\n",
    "    \n",
    "    return f_star_mean, f_star_var\n",
    "\n",
    "def RMSE_calc_rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "#plot 2d gp with x-axis as Time(UTC) dates and y axis as pm_1_ug_per_m3 values\n",
    "def plot_gp_regression(season, X, y, X_star, y_star, f_star_mean, f_star_var, kernel, **kernel_params):\n",
    "    \n",
    "    # Plot the true data\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(X, y, color='black', label='true data')\n",
    "    \n",
    "    # Plot the training/test points (subset of true data)\n",
    "    plt.plot(X_star, y_star, color='blue', label='True Function')\n",
    "    \n",
    "    # Plot the mean of the posterior predictive distribution\n",
    "    plt.plot(X_star, f_star_mean, color='red', label='Posterior Mean')\n",
    "    \n",
    "    # Plot the 95% confidence interval\n",
    "    f_star_std = np.sqrt(np.diag(f_star_var)).reshape(-1,1)  # Extracting standard deviation\n",
    "    plt.fill_between(X_star.ravel(), \n",
    "                    (f_star_mean - 1.96 * f_star_std).ravel(), \n",
    "                    (f_star_mean + 1.96 * f_star_std).ravel(), \n",
    "                     color='red', alpha=0.2, label='95% Confidence Interval')\n",
    "    \n",
    "    plt.xlabel(season)\n",
    "    plt.ylabel('pm_1_ug_per_m3')\n",
    "    plt.title(f'GP Regression with {kernel.__name__}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "running the gp with various datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_run_gp_RSME_plot(season, X, y, n, x_axis_feature, y_axis_value, kernel, **kernel_params):\n",
    "    X_subset, y_subset = extract_subset(X, y, n)\n",
    "\n",
    "    f_mean_star, f_var_star = run_gp_regression(X_subset, y_subset, X, kernel, **kernel_params)\n",
    "    \n",
    "    #print RSME\n",
    "    print(f'RMSE: {RMSE_calc_rmse(y_axis_value.values, f_mean_star)}')\n",
    "\n",
    "    plot_gp_regression(season, x_axis_feature.values, y_axis_value.values, x_axis_feature.values , y_axis_value.values, f_mean_star, f_var_star, kernel, **kernel_params)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" #extracting and plotting gp regression for s2_{season}_daily data set, autumn, winter, summer, spring\n",
    "n = 30\n",
    "X = s2_autunm_daily[features].values\n",
    "y = s2_autunm_daily['pm_1_ug_per_m3'].values\n",
    "X, y = np.array(X), y.reshape(-1, 1)\n",
    "\n",
    "# sum kernel\n",
    "extract_run_gp_RSME_plot('autumn', X, y, n, s2_autunm_daily['Time(UTC)'], s2_autunm_daily['pm_1_ug_per_m3'], sum_kernel, length_scale=2.5, nu=1.5, variance=5.0, period=1.0)\n",
    "\n",
    "X = s2_winter_daily[features].values\n",
    "y = s2_winter_daily['pm_1_ug_per_m3'].values\n",
    "X, y = np.array(X), y.reshape(-1, 1)\n",
    "\n",
    "extract_run_gp_RSME_plot('winter', X, y, n, s2_winter_daily['Time(UTC)'], s2_winter_daily['pm_1_ug_per_m3'], sum_kernel, length_scale=2.5, nu=1.5, variance=5.0, period=1.0)\n",
    "\n",
    "X = s2_summer_daily[features].values\n",
    "y = s2_summer_daily['pm_1_ug_per_m3'].values\n",
    "X, y = np.array(X), y.reshape(-1, 1)\n",
    "\n",
    "extract_run_gp_RSME_plot('summer', X, y, n, s2_summer_daily['Time(UTC)'], s2_summer_daily['pm_1_ug_per_m3'], sum_kernel, length_scale=2.5, nu=1.5, variance=1.0, period=1.0)\n",
    "\n",
    "X = s2_spring_daily[features].values\n",
    "y = s2_spring_daily['pm_1_ug_per_m3'].values\n",
    "X, y = np.array(X), y.reshape(-1, 1)\n",
    "\n",
    "extract_run_gp_RSME_plot('spring', X, y, n, s2_spring_daily['Time(UTC)'], s2_spring_daily['pm_1_ug_per_m3'], sum_kernel, length_scale=2.5, nu=1.5, variance=5.0, period=1.0) \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysisV1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
