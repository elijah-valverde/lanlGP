{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Proccess (GP) on M1 and S2 Site Data\n",
    "We wish to determine the seasonal and diurnal cycles of supermicron aerosols/bioaerosols. In this notebook we focus on the seasonal trends. We will split the data up by seasons and fit a GP on a subset of the data. Using a confidence interval, we will determine if the data indicates seasonal cycles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file paths\n",
    "file_m1 = r\"C:\\Users\\396760\\lanl_data\\PM_clean\\ARMSAILM1_cleaned.csv\"\n",
    "file_s2 = r'C:\\Users\\396760\\lanl_data\\PM_clean\\ARMSAILS2_cleaned.csv'\n",
    "# Load the data\n",
    "m1data = pd.read_csv(file_m1)\n",
    "s2data = pd.read_csv(file_s2)\n",
    "\n",
    "# Convert timestamps to datetime format\n",
    "m1data['Time(UTC)'] = pd.to_datetime(m1data['Time(UTC)'])\n",
    "s2data['Time(UTC)'] = pd.to_datetime(s2data['Time(UTC)'])\n",
    "\n",
    "# Handle missing values (NaN); fill with mean value\n",
    "m1data.fillna(m1data.mean(), inplace=True)\n",
    "s2data.fillna(s2data.mean(), inplace=True)\n",
    "\n",
    "# Separate data by seasons for M1 data\n",
    "winterM1 = m1data[(m1data['Time(UTC)'] >= '2022-12-21') & (m1data['Time(UTC)'] < '2023-03-20')]\n",
    "springM1 = m1data[(m1data['Time(UTC)'] >= '2023-03-21') & (m1data['Time(UTC)'] < '2023-06-20')]  \n",
    "\n",
    "#-----------------------------------------------------------------------------------------------#\n",
    "\n",
    "#subset of collumns for PM1 data\n",
    "collumnsPM1 = ['sample_rh_pct', 'sample_temp_C', 'sample_pres_mmHg', 'pm_1_ug_per_m3']\n",
    "#subset of collumns for PM2.5 data\n",
    "collumnsPM25 = ['sample_rh_pct', 'sample_temp_C', 'sample_pres_mmHg', 'pm_2_5_ug_per_m3']\n",
    "#subset of collumns for pm10 data\n",
    "collumnsPM10 = ['sample_rh_pct', 'sample_temp_C', 'sample_pres_mmHg', 'pm_10_ug_per_m3']\n",
    "\n",
    "# Define features and target variable\n",
    "features = ['sample_rh_pct', 'sample_temp_C', 'sample_pres_mmHg']\n",
    "targetPM1= 'pm_1_ug_per_m3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling Issues\n",
    "A well known issue of Gaussian Processes is their limited scalability due to $O(n^3)$ complexity. We attempt to use a Sparse Gaussian Process (SGP) by means of Kmeans Clustering. *Note, for daily averages we do not need to use SGP since $n<1000$ for both Spring and Winter datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Function to generate inducing points\n",
    "def inducing_points(data_set, sample_size, subset, num_of_inducing_points, features, random_state):\n",
    "    subset = data_set.sample(n=sample_size, random_state=random_state)\n",
    "    kmeans = KMeans(n_clusters=num_of_inducing_points, random_state=random_state).fit(subset[features]) \n",
    "    inducing_points = kmeans.cluster_centers_ #cluster centers\n",
    "    silhouette_avg = silhouette_score(subset[features], kmeans.labels_) #score quantifying choice of clusters, closer to 1 is better\n",
    "\n",
    "    return inducing_points, silhouette_avg\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# MatÃ©rn kernel function\n",
    "def matern_kernel(X,  Y=None, length_scale=1.0, nu=1.5,):\n",
    "    \n",
    "    dists = cdist(X, Y, metric='euclidean')\n",
    "    \n",
    "    if nu == 0.5:\n",
    "        K = np.exp(-dists / length_scale)\n",
    "    elif nu == 1.5:\n",
    "        sqrt3 = np.sqrt(3)\n",
    "        K = (1.0 + sqrt3 * dists / length_scale) * np.exp(-sqrt3 * dists / length_scale)\n",
    "    elif nu == 2.5:\n",
    "        sqrt5 = np.sqrt(5)\n",
    "        K = (1.0 + sqrt5 * dists / length_scale + 5 * dists**2 / (3 * length_scale**2)) * np.exp(-sqrt5 * dists / length_scale)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported nu value. Only 0.5, 1.5, and 2.5 are supported.\")\n",
    "    \n",
    "    return K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gp_regression(X_train, y_train, X_test, noise=1e-6):\n",
    "\n",
    "    #reshaping for linear algebra operations\n",
    "    X_train = np.array(X_train)\n",
    "    X_test = np.array(X_test)   \n",
    "    X_train = X_train.reshape(-1, 1)\n",
    "    X_test = X_test.reshape(-1, 1)\n",
    "\n",
    "\n",
    "\n",
    "    print(X_train.shape, y_train.shape, X_test.shape)\n",
    "    K = matern_kernel(X_train, X_train, length_scale=1.0, nu=1.5)\n",
    "    K_star = matern_kernel(X_train, X_test, length_scale=1.0, nu=1.5)\n",
    "    K_star_star = matern_kernel(X_test, X_test, length_scale=1.0, nu=1.5)\n",
    "\n",
    "    L = np.linalg.cholesky(K + noise * np.eye(K.shape[0]))\n",
    "    temp = np.linalg.solve(L, y_train)\n",
    "    alpha = np.linalg.solve(L.T, temp)\n",
    "\n",
    "    f_star = K_star.T @ alpha\n",
    "    v = np.linalg.solve(L, K_star)\n",
    "    var_f_star = K_star_star - v.T @ v\n",
    "\n",
    "    \n",
    "    std_f_star = np.sqrt(np.diag(var_f_star))\n",
    "\n",
    "    #log_marginal_likelihood = -0.5 * y_train.T @ alpha - np.sum(np.log(np.diag(L))) - 0.5 * y_train.shape[0] * np.log(2 * np.pi)\n",
    "\n",
    "    return f_star, var_f_star, std_f_star #log_marginal_likelihood\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daily Averages\n",
    "Here we take the daily averages from each season, and run the GP on those averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\396760\\AppData\\Local\\Temp\\ipykernel_9440\\3115850127.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  winterM1.loc[:, 'Date'] = winterM1['Time(UTC)'].dt.date\n",
      "C:\\Users\\396760\\AppData\\Local\\Temp\\ipykernel_9440\\3115850127.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  springM1.loc[:, 'Date'] = springM1['Time(UTC)'].dt.date\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 1) (50,) (30, 1)\n"
     ]
    },
    {
     "ename": "DTypePromotionError",
     "evalue": "The DTypes <class 'numpy.dtypes.Float64DType'> and <class 'numpy.dtypes.DateTime64DType'> do not have a common DType. For example they cannot be stored in a single array unless the dtype is `object`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDTypePromotionError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 23\u001b[0m\n\u001b[0;32m     18\u001b[0m day_wint_feature_values \u001b[38;5;241m=\u001b[39m day_wint[targetPM1]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m     21\u001b[0m test_day \u001b[38;5;241m=\u001b[39m winterM1\u001b[38;5;241m.\u001b[39msample(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTime(UTC)\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m---> 23\u001b[0m \u001b[43mrun_gp_regression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mday_wint_feature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mday_wint_feature_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_day\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-6\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 13\u001b[0m, in \u001b[0;36mrun_gp_regression\u001b[1;34m(X_train, y_train, X_test, noise)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(X_train\u001b[38;5;241m.\u001b[39mshape, y_train\u001b[38;5;241m.\u001b[39mshape, X_test\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     12\u001b[0m K \u001b[38;5;241m=\u001b[39m matern_kernel(X_train, X_train, length_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, nu\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.5\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m K_star \u001b[38;5;241m=\u001b[39m \u001b[43mmatern_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlength_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m K_star_star \u001b[38;5;241m=\u001b[39m matern_kernel(X_test, X_test, length_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, nu\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.5\u001b[39m)\n\u001b[0;32m     16\u001b[0m L \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mcholesky(K \u001b[38;5;241m+\u001b[39m noise \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39meye(K\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]))\n",
      "Cell \u001b[1;32mIn[5], line 7\u001b[0m, in \u001b[0;36mmatern_kernel\u001b[1;34m(X, Y, length_scale, nu)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmatern_kernel\u001b[39m(X,  Y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, length_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, nu\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.5\u001b[39m,):\n\u001b[1;32m----> 7\u001b[0m     dists \u001b[38;5;241m=\u001b[39m \u001b[43mcdist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meuclidean\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nu \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.5\u001b[39m:\n\u001b[0;32m     10\u001b[0m         K \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m-\u001b[39mdists \u001b[38;5;241m/\u001b[39m length_scale)\n",
      "File \u001b[1;32mc:\\Users\\396760\\Conda\\envs\\analysisV1\\Lib\\site-packages\\scipy\\spatial\\distance.py:2980\u001b[0m, in \u001b[0;36mcdist\u001b[1;34m(XA, XB, metric, out, **kwargs)\u001b[0m\n\u001b[0;32m   2978\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metric_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2979\u001b[0m     cdist_fn \u001b[38;5;241m=\u001b[39m metric_info\u001b[38;5;241m.\u001b[39mcdist_func\n\u001b[1;32m-> 2980\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcdist_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2981\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mstr\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   2982\u001b[0m     metric_info \u001b[38;5;241m=\u001b[39m _TEST_METRICS\u001b[38;5;241m.\u001b[39mget(mstr, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mDTypePromotionError\u001b[0m: The DTypes <class 'numpy.dtypes.Float64DType'> and <class 'numpy.dtypes.DateTime64DType'> do not have a common DType. For example they cannot be stored in a single array unless the dtype is `object`."
     ]
    }
   ],
   "source": [
    "# Extract date from the datetime for both seasons\n",
    "winterM1.loc[:, 'Date'] = winterM1['Time(UTC)'].dt.date\n",
    "springM1.loc[:, 'Date'] = springM1['Time(UTC)'].dt.date\n",
    "\n",
    "# Group by date and calculate daily average for winter season\n",
    "daily_winterM1 = winterM1.groupby('Date').mean().reset_index()\n",
    "daily_winterM1PM1 = daily_winterM1[collumnsPM1]\n",
    "\n",
    "# Group by date and calculate daily average for spring season\n",
    "daily_springM1 = springM1.groupby('Date').mean().reset_index()\n",
    "daily_springM1PM1 = daily_springM1[collumnsPM1]\n",
    "\n",
    "\n",
    "#Running GP on winter data\n",
    "day_wint = daily_winterM1[collumnsPM1].sample(n=50 , random_state=42)\n",
    "\n",
    "day_wint_feature = day_wint[features]\n",
    "day_wint_feature_values = day_wint[targetPM1].values\n",
    "\n",
    "\n",
    "test_day = winterM1.sample(n=30, random_state=42)['Time(UTC)'].values\n",
    "\n",
    "run_gp_regression(day_wint_feature, day_wint_feature_values, test_day, noise=1e-6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysisV1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
