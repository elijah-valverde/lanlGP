{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gpytorch\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"2\"\n",
    "\n",
    "\n",
    "\n",
    "def data_import(path1, delimiter):\n",
    "    #locate file path and import data\n",
    "    if delimiter == 'none':\n",
    "        file1 = pd.read_csv(path1)\n",
    "    else:   \n",
    "        file1 = pd.read_csv(path1, delimiter=delimiter)\n",
    "    return file1\n",
    "\n",
    "def data_convert(file1, time_column_name: str):\n",
    "    #convert timestamps to datetime format\n",
    "    file1[time_column_name] = pd.to_datetime(file1[time_column_name])\n",
    "    return file1\n",
    "\n",
    "\n",
    "#function to remove any rows containing even a single NaN value\n",
    "def remove_nan(df):\n",
    "    df = df.replace(\"nan\", np.nan)\n",
    "    df = df.dropna()\n",
    "    return df\n",
    "\n",
    "def data_frequency(file1, desired_frequency: str, time_column_name: str):\n",
    "    if desired_frequency == 'ten_minute':\n",
    "        file1 = file1.resample('10T', on=time_column_name).mean()\n",
    "    elif desired_frequency == '4_hourly':\n",
    "        file1 = file1.resample('4h', on=time_column_name).mean()\n",
    "    elif desired_frequency == 'hourly':\n",
    "        file1 = file1.resample('h', on=time_column_name).mean()\n",
    "    elif desired_frequency == 'twelve_hourly':\n",
    "        file1 = file1.resample('12h', on=time_column_name).mean()\n",
    "    elif desired_frequency == 'daily':\n",
    "        file1 = file1.resample('D', on=time_column_name).mean()\n",
    "\n",
    "    # Ensure the index is datetime\n",
    "    file1.index = pd.to_datetime(file1.index)\n",
    "    \n",
    "    # Reset the index and name it 'Time(UTC)'\n",
    "    file1.reset_index(inplace=True)\n",
    "    file1.rename(columns={file1.index.name: time_column_name}, inplace=True)\n",
    "\n",
    "    return file1\n",
    "\n",
    "\n",
    "def time_to_sincos(df):\n",
    "    sin_values = []\n",
    "    cos_values = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        sin_values.append(np.sin((2 * np.pi * i) / 365.25))\n",
    "        cos_values.append(np.cos((2 * np.pi * i) / 365.25))\n",
    "        \n",
    "    df['Time_sin'] = sin_values\n",
    "    df['Time_cos'] = cos_values\n",
    "    \n",
    "    return df\n",
    "\n",
    "#trim data to desired collumns\n",
    "def file_trim(file1, desired_collumns):\n",
    "    file1 = file1[desired_collumns]\n",
    "    return file1\n",
    "\n",
    "#separate data by seasons\n",
    "def data_split(file1, time_column_name: str):\n",
    "    winter = file1[(file1[time_column_name] >= '2022-12-21') & (file1[time_column_name] < '2023-03-20')]\n",
    "    spring = file1[(file1[time_column_name] >= '2023-03-20') & (file1[time_column_name] < '2023-06-21')]\n",
    "    summer = file1[(file1[time_column_name] >= '2022-06-21') & (file1[time_column_name] < '2022-09-23')]\n",
    "    autunm = file1[(file1[time_column_name] >= '2022-09-23') & (file1[time_column_name] < '2022-12-21')]\n",
    "    return winter, spring, summer, autunm\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing s2 and m1 data sets\n",
    "s2_site_data= data_import(\"C:\\\\Users\\\\396760\\\\lanl\\\\data\\\\ARMSAILS2_cleaned.csv\", 'none')\n",
    "m1_site_data = data_import(\"C:\\\\Users\\\\396760\\\\lanl\\\\data\\\\ARMSAILM1_cleaned.csv\", 'none')\n",
    "\n",
    "#data conversion and handling for m1, s2 data\n",
    "s2_site_data, m1_site_data = data_convert(s2_site_data, 'Time(UTC)'), data_convert(m1_site_data, 'Time(UTC)')\n",
    "\n",
    "#defining collumns and imp feature variables\n",
    "collumns = ['Time(UTC)', 'sample_rh_pct', 'sample_temp_C', 'pm_1_ug_per_m3', 'Time_sin', 'Time_cos']\n",
    "collumns1 = ['Time(UTC)', 'sample_rh_pct', 'sample_temp_C', 'pm_25_ug_per_m3', 'Time_sin', 'Time_cos']\n",
    "features = ['sample_rh_pct', 'sample_temp_C', 'Time_sin', 'Time_cos']\n",
    "s2_target_pm1 = m1_target_pm1 = ['pm_1_ug_per_m3']\n",
    "s2_target_pm25 = m1_target_pm25 = ['pm_25_ug_per_m3']\n",
    "s2_target_pm10 = m1_target_pm10 = ['pm_10_ug_per_m3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining datasets of differing frequencies in the following order : daily, 12 hourly, 4 hourly and hourly, \n",
    "s2_site_daily, m1_site_daily = data_frequency(s2_site_data, 'daily', 'Time(UTC)'), data_frequency(m1_site_data, 'daily', 'Time(UTC)')\n",
    "s2_site_12hourly, m1_site_12hourly = data_frequency(s2_site_data, 'twelve_hourly', 'Time(UTC)'), data_frequency(m1_site_data, 'twelve_hourly', 'Time(UTC)')\n",
    "s2_site_4hourly, m1_site_4hourly = data_frequency(s2_site_data, '4_hourly', 'Time(UTC)'), data_frequency(m1_site_data, '4_hourly', 'Time(UTC)')\n",
    "s2_site_hourly, m1_site_hourly = data_frequency(s2_site_data, 'hourly', 'Time(UTC)'), data_frequency(m1_site_data, 'hourly', 'Time(UTC)')\n",
    "\n",
    "#time to sin cos conversion\n",
    "s2_site_daily, m1_site_daily = time_to_sincos(s2_site_daily), time_to_sincos(m1_site_daily)\n",
    "s2_site_12hourly, m1_site_12hourly = time_to_sincos(s2_site_12hourly), time_to_sincos(m1_site_12hourly)\n",
    "s2_site_4hourly, m1_site_4hourly = time_to_sincos(s2_site_4hourly), time_to_sincos(m1_site_4hourly)\n",
    "s2_site_hourly, m1_site_hourly = time_to_sincos(s2_site_hourly), time_to_sincos(m1_site_hourly)\n",
    "\n",
    "#dropping rows with nan values\n",
    "s2_site_daily, m1_site_daily = remove_nan(s2_site_daily), remove_nan(m1_site_daily)\n",
    "s2_site_12hourly, m1_site_12hourly = remove_nan(s2_site_12hourly), remove_nan(m1_site_12hourly)\n",
    "s2_site_4hourly, m1_site_4hourly = remove_nan(s2_site_4hourly), remove_nan(m1_site_4hourly)\n",
    "s2_site_hourly, m1_site_hourly = remove_nan(s2_site_hourly), remove_nan(m1_site_hourly)\n",
    "\n",
    "\n",
    "#splitting data by seasons\n",
    "\n",
    "#daily\n",
    "s2_winter_daily, s2_spring_daily, s2_summer_daily, s2_autunm_daily = data_split(s2_site_daily, 'Time(UTC)')\n",
    "m1_winter_daily, m1_spring_daily, m1_summer_daily, m1_autunm_daily = data_split(m1_site_daily, 'Time(UTC)')\n",
    "#12 hourly\n",
    "s2_winter_12hourly, s2_spring_12hourly, s2_summer_12hourly, s2_autunm_12hourly = data_split(s2_site_12hourly, 'Time(UTC)')\n",
    "m1_winter_12hourly, m1_spring_12hourly, m1_summer_12hourly, m1_autunm_12hourly = data_split(m1_site_12hourly, 'Time(UTC)')\n",
    "#4 hourly\n",
    "s2_winter_4hourly, s2_spring_4hourly, s2_summer_4hourly, s2_autunm_4hourly = data_split(s2_site_4hourly, 'Time(UTC)')\n",
    "m1_winter_4hourly, m1_spring_4hourly, m1_summer_4hourly, m1_autunm_4hourly = data_split(m1_site_4hourly, 'Time(UTC)')\n",
    "#hourly\n",
    "s2_winter_hourly, s2_spring_hourly, s2_summer_hourly, s2_autunm_hourly = data_split(s2_site_hourly, 'Time(UTC)')\n",
    "m1_winter_hourly, m1_spring_hourly, m1_summer_hourly, m1_autunm_hourly = data_split(m1_site_hourly, 'Time(UTC)')\n",
    "\n",
    "#plots for s2 data sites at varying frequencies (PM1)\n",
    "\n",
    "#daily\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(s2_winter_daily['Time(UTC)'], s2_winter_daily['pm_1_ug_per_m3'], label='Winter')\n",
    "plt.plot(s2_spring_daily['Time(UTC)'], s2_spring_daily['pm_1_ug_per_m3'], label='Spring')\n",
    "plt.plot(s2_summer_daily['Time(UTC)'], s2_summer_daily['pm_1_ug_per_m3'], label='Summer')\n",
    "plt.plot(s2_autunm_daily['Time(UTC)'], s2_autunm_daily['pm_1_ug_per_m3'], label='Autunm')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#12 hourly\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(s2_winter_12hourly['Time(UTC)'], s2_winter_12hourly['pm_1_ug_per_m3'], label='Winter')\n",
    "plt.plot(s2_spring_12hourly['Time(UTC)'], s2_spring_12hourly['pm_1_ug_per_m3'], label='Spring')\n",
    "plt.plot(s2_summer_12hourly['Time(UTC)'], s2_summer_12hourly['pm_1_ug_per_m3'], label='Summer')\n",
    "plt.plot(s2_autunm_12hourly['Time(UTC)'], s2_autunm_12hourly['pm_1_ug_per_m3'], label='Autunm')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#4 hourly\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(s2_winter_4hourly['Time(UTC)'], s2_winter_4hourly['pm_1_ug_per_m3'], label='Winter')\n",
    "plt.plot(s2_spring_4hourly['Time(UTC)'], s2_spring_4hourly['pm_1_ug_per_m3'], label='Spring')\n",
    "plt.plot(s2_summer_4hourly['Time(UTC)'], s2_summer_4hourly['pm_1_ug_per_m3'], label='Summer')\n",
    "plt.plot(s2_autunm_4hourly['Time(UTC)'], s2_autunm_4hourly['pm_1_ug_per_m3'], label='Autunm')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#hourly\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(s2_winter_hourly['Time(UTC)'], s2_winter_hourly['pm_1_ug_per_m3'], label='Winter')\n",
    "plt.plot(s2_spring_hourly['Time(UTC)'], s2_spring_hourly['pm_1_ug_per_m3'], label='Spring')\n",
    "plt.plot(s2_summer_hourly['Time(UTC)'], s2_summer_hourly['pm_1_ug_per_m3'], label='Summer')\n",
    "plt.plot(s2_autunm_hourly['Time(UTC)'], s2_autunm_hourly['pm_1_ug_per_m3'], label='Autunm')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_subset(X, y,n):\n",
    "    # Number of points for training\n",
    "    n = int(n * len(X) * 0.01)\n",
    "    # Randomly shuffle the data\n",
    "    indices = np.random.RandomState(42).permutation(len(X))\n",
    "    # Select training and testing indices\n",
    "    train_indices = indices[:n]\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, Y_train = X[train_indices], y[train_indices]\n",
    "    x_test, y_test = X, y\n",
    "    return X_train, x_test, Y_train , y_test\n",
    "\n",
    "#function to conver to torch tensors\n",
    "def to_tensor(x_train, x_test, y_train, y_test):\n",
    "    x_train, x_test = torch.tensor(x_train, dtype=torch.float32), torch.tensor(x_test, dtype=torch.float32)\n",
    "    y_train, y_test = torch.tensor(y_train, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32)\n",
    "    # Squeeze to ensure correct dimensions\n",
    "    if y_train.dim() > 1:\n",
    "        y_train = y_train.squeeze(1)\n",
    "    if y_test.dim() > 1:\n",
    "        y_test = y_test.squeeze(1)\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ExactGPModel class\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, x_train, y_train, likelihood):\n",
    "        super(ExactGPModel, self).__init__(x_train, y_train, likelihood)\n",
    "        self.mean_module = gpytorch.means.ZeroMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(ard_num_dims=4, lengthscale_prior=gpytorch.priors.SmoothedBoxPrior(0.1, 2.0, sigma=0.1)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        \n",
    "        print(\"Mean:\", mean_x)\n",
    "        print(\"Covariance Matrix:\", covar_x)\n",
    "    \n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# Function to train the GP model\n",
    "def train_gp_model(x_train, y_train, training_iter=50, lr=0.01):\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    model = ExactGPModel(x_train, y_train, likelihood)\n",
    "\n",
    "    # Train the model\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    losses = []\n",
    "    for _ in range(training_iter):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x_train)\n",
    "        loss = -mll(output, y_train)\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    " \n",
    "        \n",
    "    # Print the noise variance after training\n",
    "    print(f'Final noise variance: {model.likelihood.noise.item():.3f}')\n",
    "    return model, likelihood\n",
    "\n",
    "# Function to make predictions\n",
    "def predict_gp_model(model, likelihood, x_test):\n",
    "    \n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        observed_pred = likelihood(model(x_test))\n",
    "    \n",
    "    return observed_pred\n",
    "\n",
    "def calculate_rmse(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return rmse\n",
    "\n",
    "# Function to create and display the table\n",
    "def create_and_display_table(dates, x_test, y_test, y_pred):\n",
    "    # Combine all data into a DataFrame\n",
    "\n",
    "\n",
    "    data = pd.DataFrame({\n",
    "        'Date': dates,\n",
    "        'Feature Values': list(x_test),\n",
    "        'Actual Aerosol Output': y_test,\n",
    "        'Predicted Aerosol Output': y_pred\n",
    "    })\n",
    "    \n",
    "    # Sort the table by dates\n",
    "    data_sorted = data.sort_values(by='Date')\n",
    "    \n",
    "    # Display the table\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    print(data_sorted)\n",
    "\n",
    "\n",
    "#run GP pipleline\n",
    "def run_gp_pipeline(x_train, x_test, y_train, y_test, training_iter=50, lr=0.01):\n",
    "    # Train the GP model\n",
    "    model, likelihood = train_gp_model(x_train, y_train, training_iter=training_iter, lr=lr)\n",
    "    \n",
    "    # Make predictions\n",
    "    observed_pred = predict_gp_model(model, likelihood, x_test)\n",
    "    \n",
    "    # Calculate the RMSE\n",
    "    rmse = calculate_rmse(y_test, observed_pred.mean)\n",
    "    \n",
    "    return model, likelihood, observed_pred, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24, 4) (81, 4) (24, 1) (81, 1)\n",
      "Mean: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Covariance Matrix: <gpytorch.lazy.lazy_evaluated_kernel_tensor.LazyEvaluatedKernelTensor object at 0x0000023E977903D0>\n",
      "Mean: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Covariance Matrix: <gpytorch.lazy.lazy_evaluated_kernel_tensor.LazyEvaluatedKernelTensor object at 0x0000023E97793090>\n",
      "Mean: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Covariance Matrix: <gpytorch.lazy.lazy_evaluated_kernel_tensor.LazyEvaluatedKernelTensor object at 0x0000023E977932D0>\n",
      "Mean: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Covariance Matrix: <gpytorch.lazy.lazy_evaluated_kernel_tensor.LazyEvaluatedKernelTensor object at 0x0000023E977D4CD0>\n",
      "Mean: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Covariance Matrix: <gpytorch.lazy.lazy_evaluated_kernel_tensor.LazyEvaluatedKernelTensor object at 0x0000023E977B4A50>\n",
      "Mean: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Covariance Matrix: <gpytorch.lazy.lazy_evaluated_kernel_tensor.LazyEvaluatedKernelTensor object at 0x0000023E977D61D0>\n",
      "Mean: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Covariance Matrix: <gpytorch.lazy.lazy_evaluated_kernel_tensor.LazyEvaluatedKernelTensor object at 0x0000023E977D5010>\n",
      "Mean: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Covariance Matrix: <gpytorch.lazy.lazy_evaluated_kernel_tensor.LazyEvaluatedKernelTensor object at 0x0000023EA0C1ED50>\n",
      "Mean: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Covariance Matrix: <gpytorch.lazy.lazy_evaluated_kernel_tensor.LazyEvaluatedKernelTensor object at 0x0000023E97790090>\n",
      "Mean: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Covariance Matrix: <gpytorch.lazy.lazy_evaluated_kernel_tensor.LazyEvaluatedKernelTensor object at 0x0000023E977D7F90>\n",
      "Mean: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Covariance Matrix: <gpytorch.lazy.lazy_evaluated_kernel_tensor.LazyEvaluatedKernelTensor object at 0x0000023E9A899490>\n",
      "Mean: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Covariance Matrix: <gpytorch.lazy.lazy_evaluated_kernel_tensor.LazyEvaluatedKernelTensor object at 0x0000023E977D7650>\n",
      "Mean: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Covariance Matrix: <gpytorch.lazy.lazy_evaluated_kernel_tensor.LazyEvaluatedKernelTensor object at 0x0000023E9A9562D0>\n",
      "Mean: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Covariance Matrix: <gpytorch.lazy.lazy_evaluated_kernel_tensor.LazyEvaluatedKernelTensor object at 0x0000023E977D5010>\n",
      "Mean: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Covariance Matrix: <gpytorch.lazy.lazy_evaluated_kernel_tensor.LazyEvaluatedKernelTensor object at 0x0000023E977B4A50>\n",
      "Mean: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Covariance Matrix: <gpytorch.lazy.lazy_evaluated_kernel_tensor.LazyEvaluatedKernelTensor object at 0x0000023E977D61D0>\n",
      "Mean: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Covariance Matrix: <gpytorch.lazy.lazy_evaluated_kernel_tensor.LazyEvaluatedKernelTensor object at 0x0000023E977D7650>\n",
      "Mean: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Covariance Matrix: <gpytorch.lazy.lazy_evaluated_kernel_tensor.LazyEvaluatedKernelTensor object at 0x0000023E97073D10>\n",
      "Mean: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Covariance Matrix: <gpytorch.lazy.lazy_evaluated_kernel_tensor.LazyEvaluatedKernelTensor object at 0x0000023E977B4A50>\n",
      "Mean: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Covariance Matrix: <gpytorch.lazy.lazy_evaluated_kernel_tensor.LazyEvaluatedKernelTensor object at 0x0000023E9A899490>\n",
      "Mean: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Covariance Matrix: <gpytorch.lazy.lazy_evaluated_kernel_tensor.LazyEvaluatedKernelTensor object at 0x0000023E977D7650>\n",
      "Mean: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Covariance Matrix: <gpytorch.lazy.lazy_evaluated_kernel_tensor.LazyEvaluatedKernelTensor object at 0x0000023E97073D10>\n",
      "Mean: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Covariance Matrix: <gpytorch.lazy.lazy_evaluated_kernel_tensor.LazyEvaluatedKernelTensor object at 0x0000023E977D61D0>\n",
      "Mean: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Covariance Matrix: <gpytorch.lazy.lazy_evaluated_kernel_tensor.LazyEvaluatedKernelTensor object at 0x0000023E977D5010>\n",
      "Mean: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Covariance Matrix: <gpytorch.lazy.lazy_evaluated_kernel_tensor.LazyEvaluatedKernelTensor object at 0x0000023E9A899490>\n",
      "Mean: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Covariance Matrix: <gpytorch.lazy.lazy_evaluated_kernel_tensor.LazyEvaluatedKernelTensor object at 0x0000023E977D5010>\n",
      "Mean: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Covariance Matrix: <gpytorch.lazy.lazy_evaluated_kernel_tensor.LazyEvaluatedKernelTensor object at 0x0000023E97073D10>\n",
      "Mean: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Covariance Matrix: <gpytorch.lazy.lazy_evaluated_kernel_tensor.LazyEvaluatedKernelTensor object at 0x0000023E977D5010>\n",
      "Mean: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Covariance Matrix: <gpytorch.lazy.lazy_evaluated_kernel_tensor.LazyEvaluatedKernelTensor object at 0x0000023E977B5FD0>\n",
      "Mean: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Covariance Matrix: <gpytorch.lazy.lazy_evaluated_kernel_tensor.LazyEvaluatedKernelTensor object at 0x0000023E977D61D0>\n",
      "Mean: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Covariance Matrix: <gpytorch.lazy.lazy_evaluated_kernel_tensor.LazyEvaluatedKernelTensor object at 0x0000023E977D4CD0>\n",
      "Mean: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Covariance Matrix: <gpytorch.lazy.lazy_evaluated_kernel_tensor.LazyEvaluatedKernelTensor object at 0x0000023E977B5FD0>\n",
      "Mean: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Covariance Matrix: <gpytorch.lazy.lazy_evaluated_kernel_tensor.LazyEvaluatedKernelTensor object at 0x0000023E977D7650>\n",
      "Mean: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Covariance Matrix: <gpytorch.lazy.lazy_evaluated_kernel_tensor.LazyEvaluatedKernelTensor object at 0x0000023E977D5010>\n",
      "Mean: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Covariance Matrix: <gpytorch.lazy.lazy_evaluated_kernel_tensor.LazyEvaluatedKernelTensor object at 0x0000023E977B4850>\n",
      "Mean: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Covariance Matrix: <gpytorch.lazy.lazy_evaluated_kernel_tensor.LazyEvaluatedKernelTensor object at 0x0000023E977D7650>\n",
      "Mean: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Covariance Matrix: <gpytorch.lazy.lazy_evaluated_kernel_tensor.LazyEvaluatedKernelTensor object at 0x0000023E977B5FD0>\n",
      "Mean: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Covariance Matrix: <gpytorch.lazy.lazy_evaluated_kernel_tensor.LazyEvaluatedKernelTensor object at 0x0000023E977D5C90>\n",
      "Mean: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Covariance Matrix: <gpytorch.lazy.lazy_evaluated_kernel_tensor.LazyEvaluatedKernelTensor object at 0x0000023E977D7650>\n",
      "Mean: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Covariance Matrix: <gpytorch.lazy.lazy_evaluated_kernel_tensor.LazyEvaluatedKernelTensor object at 0x0000023E977B5FD0>\n",
      "Mean: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Covariance Matrix: <gpytorch.lazy.lazy_evaluated_kernel_tensor.LazyEvaluatedKernelTensor object at 0x0000023E977D5010>\n",
      "Mean: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Covariance Matrix: <gpytorch.lazy.lazy_evaluated_kernel_tensor.LazyEvaluatedKernelTensor object at 0x0000023E977D4CD0>\n",
      "Mean: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Covariance Matrix: <gpytorch.lazy.lazy_evaluated_kernel_tensor.LazyEvaluatedKernelTensor object at 0x0000023E977D7C50>\n",
      "Mean: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Covariance Matrix: <gpytorch.lazy.lazy_evaluated_kernel_tensor.LazyEvaluatedKernelTensor object at 0x0000023E977D7F90>\n",
      "Mean: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Covariance Matrix: <gpytorch.lazy.lazy_evaluated_kernel_tensor.LazyEvaluatedKernelTensor object at 0x0000023E977D7C50>\n",
      "Mean: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Covariance Matrix: <gpytorch.lazy.lazy_evaluated_kernel_tensor.LazyEvaluatedKernelTensor object at 0x0000023E97792B90>\n",
      "Mean: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Covariance Matrix: <gpytorch.lazy.lazy_evaluated_kernel_tensor.LazyEvaluatedKernelTensor object at 0x0000023E97792F50>\n",
      "Mean: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Covariance Matrix: <gpytorch.lazy.lazy_evaluated_kernel_tensor.LazyEvaluatedKernelTensor object at 0x0000023E977B4E50>\n",
      "Mean: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Covariance Matrix: <gpytorch.lazy.lazy_evaluated_kernel_tensor.LazyEvaluatedKernelTensor object at 0x0000023E97792B90>\n",
      "Mean: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Covariance Matrix: <gpytorch.lazy.lazy_evaluated_kernel_tensor.LazyEvaluatedKernelTensor object at 0x0000023E977D7650>\n",
      "Final noise variance: 0.955\n",
      "Mean: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Covariance Matrix: <gpytorch.lazy.lazy_evaluated_kernel_tensor.LazyEvaluatedKernelTensor object at 0x0000023E977D7650>\n",
      "Mean: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Covariance Matrix: <gpytorch.lazy.lazy_evaluated_kernel_tensor.LazyEvaluatedKernelTensor object at 0x0000023E97789890>\n",
      "          Date                                     Feature Values  \\\n",
      "190 2022-12-21  [tensor(68.2358), tensor(-6.2317), tensor(-0.1...   \n",
      "191 2022-12-22  [tensor(65.1583), tensor(-8.2929), tensor(-0.1...   \n",
      "192 2022-12-23  [tensor(41.5442), tensor(-5.7958), tensor(-0.1...   \n",
      "193 2022-12-24  [tensor(61.1404), tensor(-3.3817), tensor(-0.1...   \n",
      "194 2022-12-25  [tensor(64.7294), tensor(-2.4948), tensor(-0.1...   \n",
      "..         ...                                                ...   \n",
      "274 2023-03-15  [tensor(69.1791), tensor(0.9634), tensor(-1.00...   \n",
      "275 2023-03-16  [tensor(62.4865), tensor(-0.2483), tensor(-0.9...   \n",
      "276 2023-03-17  [tensor(52.5157), tensor(-4.7834), tensor(-0.9...   \n",
      "277 2023-03-18  [tensor(25.2033), tensor(-3.1293), tensor(-0.9...   \n",
      "278 2023-03-19  [tensor(29.2801), tensor(-4.1915), tensor(-0.9...   \n",
      "\n",
      "     Actual Aerosol Output                   Predicted Aerosol Output  \n",
      "190              12.232763  MultivariateNormal(loc: torch.Size([81]))  \n",
      "191              12.664039  MultivariateNormal(loc: torch.Size([81]))  \n",
      "192               9.784573  MultivariateNormal(loc: torch.Size([81]))  \n",
      "193              12.723360  MultivariateNormal(loc: torch.Size([81]))  \n",
      "194              10.246017  MultivariateNormal(loc: torch.Size([81]))  \n",
      "..                     ...                                        ...  \n",
      "274               3.970444  MultivariateNormal(loc: torch.Size([81]))  \n",
      "275              15.957257  MultivariateNormal(loc: torch.Size([81]))  \n",
      "276              13.069872  MultivariateNormal(loc: torch.Size([81]))  \n",
      "277               8.247614  MultivariateNormal(loc: torch.Size([81]))  \n",
      "278               9.750790  MultivariateNormal(loc: torch.Size([81]))  \n",
      "\n",
      "[81 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Extracting the subset of data for training and testing\n",
    "x_train_windaily, x_test_windaily, y_train_windaily, y_test_windaily = extract_subset(s2_winter_daily[features].values, \n",
    "                                                                                      s2_winter_daily[s2_target_pm1].values,30)\n",
    "print(x_train_windaily.shape, x_test_windaily.shape, y_train_windaily.shape, y_test_windaily.shape)\n",
    "\n",
    "# Convert to torch tensors\n",
    "x_train_windaily, x_test_windaily, y_train_windaily, y_test_windaily = to_tensor(x_train_windaily, x_test_windaily,\n",
    "                                                                                 y_train_windaily, y_test_windaily)\n",
    "\n",
    "#run GP\n",
    "model_windaily, likelihood_windaily, observed_pred_windaily, rmse_windaily = run_gp_pipeline(x_train_windaily, x_test_windaily, y_train_windaily, y_test_windaily)\n",
    "\n",
    "# Create and display the table\n",
    "create_and_display_table(s2_winter_daily['Time(UTC)'], x_test_windaily, y_test_windaily, observed_pred_windaily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysisV1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
