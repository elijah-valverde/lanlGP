{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gpytorch \n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"2\"\n",
    "\n",
    "def data_import(path1, delimiter):\n",
    "    #locate file path and import data\n",
    "    if delimiter == 'none':\n",
    "        file1 = pd.read_csv(path1)\n",
    "    else:   \n",
    "        file1 = pd.read_csv(path1, delimiter=delimiter)\n",
    "    return file1\n",
    "\n",
    "\n",
    "def data_frequency(file1, desired_frequency: str):\n",
    "    if desired_frequency == 'ten_minute':\n",
    "        file1 = file1.resample('10T', on='Time(UTC)').mean()\n",
    "    elif desired_frequency == '4_hourly':\n",
    "        file1 = file1.resample('4h', on='Time(UTC)').mean()\n",
    "    elif desired_frequency == 'hourly':\n",
    "        file1 = file1.resample('h', on='Time(UTC)').mean()\n",
    "    elif desired_frequency == 'twelve_hourly':\n",
    "        file1 = file1.resample('12h', on='Time(UTC)').mean()\n",
    "    elif desired_frequency == 'daily':\n",
    "        file1 = file1.resample('d', on='Time(UTC)').mean()\n",
    "\n",
    "    # Ensure the index is datetime\n",
    "    file1.index = pd.to_datetime(file1.index)\n",
    "    \n",
    "    # Reset the index and name it 'Time(UTC)'\n",
    "    file1.reset_index(inplace=True)\n",
    "    file1.rename(columns={file1.index.name: 'Time(UTC)'}, inplace=True)\n",
    "\n",
    "    return file1\n",
    "\n",
    "def time_to_sincos(df):\n",
    "    sin_values = []\n",
    "    cos_values = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        sin_values.append(np.sin((2 * np.pi * i) / 365.25))\n",
    "        cos_values.append(np.cos((2 * np.pi * i) / 365.25))\n",
    "        \n",
    "    df['Time_sin'] = sin_values\n",
    "    df['Time_cos'] = cos_values\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing s2 and m1 data sets\n",
    "s2_site_data= data_import(\"C:\\\\Users\\\\396760\\\\lanl\\\\data\\\\ARMSAILS2_cleaned.csv\", 'none')\n",
    "m1_site_data = data_import(\"C:\\\\Users\\\\396760\\\\lanl\\\\data\\\\ARMSAILM1_cleaned.csv\", 'none')\n",
    "\n",
    "#remove pressure data from sets\n",
    "s2_site_data.drop(columns=['sample_pres_mmHg'], inplace=True)\n",
    "m1_site_data.drop(columns=['sample_pres_mmHg'], inplace=True)\n",
    "\n",
    "#drop rows with even just one nan value\n",
    "s2_site_data.dropna(inplace=True)\n",
    "m1_site_data.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" #ensure the nan's are removed\\nprint(s2_site_data.isnull().sum())\\nprint(m1_site_data.isnull().sum())\\n \""
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#append sine and cosine values to the data\n",
    "s2_site_data = time_to_sincos(s2_site_data)\n",
    "m1_site_data = time_to_sincos(m1_site_data)\n",
    "\n",
    "#drop entire rows with even just one NaN value\n",
    "s2_site_data.dropna(inplace=True)\n",
    "m1_site_data.dropna(inplace=True)\n",
    "\n",
    "# 'Time(UTC)' column to dt format\n",
    "s2_site_data['Time(UTC)'] = pd.to_datetime(s2_site_data['Time(UTC)'])\n",
    "m1_site_data['Time(UTC)'] = pd.to_datetime(m1_site_data['Time(UTC)'])\n",
    "\n",
    "\"\"\" #ensure the nan's are removed\n",
    "print(s2_site_data.isnull().sum())\n",
    "print(m1_site_data.isnull().sum())\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" \\nfor i in range(1, 13):\\n    check_dim(globals()['s2_month' + str(i)])\\n    check_dim(globals()['m1_month' + str(i)])   \\n \""
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"the data was collected over a 12 month span. partition the data into monthly sets. to partition into different sets, use variables in above cell\"\n",
    "\n",
    "#s2_month(i), where i is the number of month.\n",
    "def partition_data(data, start_month, end_month):\n",
    "    data = data.loc[(data['Time(UTC)'].dt.month >= start_month) & (data['Time(UTC)'].dt.month <= end_month)]\n",
    "    return data\n",
    "\n",
    "for i in range(1, 13):\n",
    "    globals()['s2_month' + str(i)] = partition_data(s2_site_data, i, i)\n",
    "    globals()['m1_month' + str(i)] = partition_data(m1_site_data, i, i)\n",
    "\n",
    "#remove the 'Time(UTC)' column from the data\n",
    "for i in range(1, 13):\n",
    "    s2_month = globals()['s2_month' + str(i)].drop(columns=['Time(UTC)'])\n",
    "    m1_month = globals()['m1_month' + str(i)].drop(columns=['Time(UTC)'])\n",
    "    globals()['s2_month' + str(i)] = s2_month\n",
    "    globals()['m1_month' + str(i)] = m1_month\n",
    "    \n",
    "\"\"\" def check_dim(data):\n",
    "    print(data.shape) \"\"\"\n",
    "\n",
    "\"\"\" #access pm1 concentration collumn, calculate mean and print\n",
    "def mean_pm1(data):\n",
    "    pm1 = data['pm_1_ug_per_m3']\n",
    "    mean = np.mean(pm1)\n",
    "    return mean \"\"\"\n",
    "\n",
    "\"\"\" for i in range(1,13):\n",
    "    s2_totalmean = mean_pm1(globals()['s2_month' + str(i)])\n",
    "    m1_totalmean = mean_pm1(globals()['m1_month' + str(i)])\n",
    "\n",
    "    print(mean_pm1(globals()['s2_month' + str(i)]))\n",
    "\n",
    "    print(mean_pm1(globals()['m1_month' + str(i)]))\n",
    "\n",
    "print('avg mean here')\n",
    "print(s2_totalmean/12)\n",
    "print(m1_totalmean/12) \"\"\"\n",
    "\n",
    "\"\"\" for i in range(1, 13):\n",
    "    check_dim(globals()['s2_month' + str(i)])\n",
    "    check_dim(globals()['m1_month' + str(i)]) \"\"\"\n",
    "\n",
    "#gpytorch requires data to be in tensor format\n",
    "def to_tensor(data):\n",
    "    data = torch.tensor(data.values)\n",
    "    return data\n",
    "\n",
    "for i in range(1, 13):\n",
    "    globals()['s2_month' + str(i)] = to_tensor(globals()['s2_month' + str(i)])\n",
    "    globals()['m1_month' + str(i)] = to_tensor(globals()['m1_month' + str(i)])\n",
    "\n",
    "\"\"\" \n",
    "for i in range(1, 13):\n",
    "    check_dim(globals()['s2_month' + str(i)])\n",
    "    check_dim(globals()['m1_month' + str(i)])   \n",
    " \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ZeroMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "\n",
    "# Initialize Gaussian likelihood\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = ExactGPModel(train_x, train_y, likelihood)\n",
    "training_iter = 50\n",
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "# Use the adam optimizer, includes GaussianLikelihood parameters\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "# Set our loss as the negative log GP marginal likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(training_iter):\n",
    "    # Zero gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    # Output from model\n",
    "    output = model(train_x)\n",
    "    # Calc loss and backprop gradients\n",
    "    loss = -mll(output, train_y)\n",
    "    loss.backward()\n",
    "    if i % 10 == 0:\n",
    "        print(f'Iter {i+1:d}/{training_iter:d} - Loss: {loss.item():.3f} '\n",
    "              f'squared lengthscale: '\n",
    "              f'{model.covar_module.base_kernel.lengthscale.item():.3f} '\n",
    "              f'noise variance: {model.likelihood.noise.item():.3f}')\n",
    "    optimizer.step()\n",
    "\n",
    "test_x = torch.tensor(test_x)\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "observed_pred = likelihood(model(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/200 - Loss: 20.129207611083984\n",
      "Iter 11/200 - Loss: 17.84337043762207\n",
      "Iter 21/200 - Loss: 15.242698669433594\n",
      "Iter 31/200 - Loss: 13.218689918518066\n",
      "Iter 41/200 - Loss: 11.792628288269043\n",
      "Iter 51/200 - Loss: 10.858305931091309\n",
      "Iter 61/200 - Loss: 10.149012565612793\n",
      "Iter 71/200 - Loss: 9.597676277160645\n",
      "Iter 81/200 - Loss: 9.166234016418457\n",
      "Iter 91/200 - Loss: 8.80482292175293\n",
      "Iter 101/200 - Loss: 8.498921394348145\n",
      "Iter 111/200 - Loss: 8.235713958740234\n",
      "Iter 121/200 - Loss: 8.010125160217285\n",
      "Iter 131/200 - Loss: 7.810035228729248\n",
      "Iter 141/200 - Loss: 7.627243518829346\n",
      "Iter 151/200 - Loss: 7.463432312011719\n",
      "Iter 161/200 - Loss: 7.323135852813721\n",
      "Iter 171/200 - Loss: 7.1839680671691895\n",
      "Iter 181/200 - Loss: 7.06618595123291\n",
      "Iter 191/200 - Loss: 6.9524641036987305\n",
      "Final noise variance: 5.483\n"
     ]
    }
   ],
   "source": [
    "# Ensure that your 'Time(UTC)' column is already in datetime format\n",
    "s2_site_data['Time(UTC)'] = pd.to_datetime(s2_site_data['Time(UTC)'])\n",
    "\n",
    "# Resample the data to daily frequency\n",
    "s2_daily = data_frequency(s2_site_data, 'daily')\n",
    "\n",
    "# Set the target variable and feature variables\n",
    "target_var = 'pm_1_ug_per_m3'\n",
    "feature_vars = ['Time_sin', 'Time_cos', 'sample_rh_pct', 'sample_temp_C']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to tensors\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "trained_model, likelihood = train_gp_model(X_train_tensor, y_train_tensor)\n",
    "observed_pred_train = predict_gp_model(trained_model,likelihood, X_test_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analysisV1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
